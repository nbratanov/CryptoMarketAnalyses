import torch
import torch.nn as nn
import pandas as pd
import numpy as np
import re
import spacy
from collections import Counter
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
import string
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
from sklearn.metrics import mean_squared_error
import time
from src.common.utils import Utils


dataset = pd.read_csv("../data/mergedData.csv")
#dataset = dataset.drop_duplicates(subset='message', keep="last")
#dataset = dataset[dataset['date'].apply(lambda x:  pd.to_datetime(x) > pd.to_datetime('2019-01-01 00:00:00+00:00'))]
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

dataset = dataset[['message', 'movement_the_day_after', 'Close']]
dataset.columns = ['message', 'movement_the_day_after', 'close']
dataset['output'] = 100 * dataset['movement_the_day_after'] / dataset['close']
dataset['output'] = dataset['output'].apply(lambda x: Utils.get_class(x))
dataset['message_length'] = dataset['message'].apply(lambda x: len(x.split()))

#tokenization
tok = spacy.load('en_core_web_sm')
#tok = TweetTokenizer(preserve_case=False, strip_handles=True)
#vectorizer = TfidfVectorizer(tokenizer=tok.tokenize, max_features = 20, ngram_range=(1, 2))


def tokenize (text):
    text = re.sub(r"[^\x00-\x7F]+", " ", text)
    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\r\\t\\n]') # remove punctuation and numbers
    nopunct = regex.sub(" ", text.lower())
    return [token.text for token in tok.tokenizer(nopunct)]
    #return tok.tokenize(nopunct)

counts = Counter()
for index, row in dataset.iterrows():
    counts.update(tokenize(row['message']))

#deleting infrequent words
print("num_words before:",len(counts.keys()))
for word in list(counts):
    if counts[word] < 2:
        del counts[word]
print("num_words after:",len(counts.keys()))

#creating vocabulary
vocab2index = {"":0, "UNK":1}
words = ["", "UNK"]
for word in counts:
    vocab2index[word] = len(words)
    words.append(word)

def get_encoded_sentence(text, vocab2index, N=35):
    tokenized_sentence = tokenize(text)
    encoded = np.zeros(N, dtype=int)
    enc1 = np.array([vocab2index.get(word, vocab2index["UNK"]) for word in tokenized_sentence])
    length = min(N, len(enc1))
    encoded[:length] = enc1[:length]
    return encoded, length

dataset['encoded'] = dataset['message'].apply(lambda x: np.array(get_encoded_sentence(x,vocab2index )))

X = list(dataset['encoded'])
y = list(dataset['output'])

X_train_all, X_test, y_train_all, y_test = train_test_split(X, y, test_size=0.1, shuffle=True)
X_train, X_valid, y_train, y_valid = train_test_split(X_train_all, y_train_all, test_size=0.1)



class Dataset(Dataset):
    def __init__(self, X, Y):
        self.X = X
        self.y = Y

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]


train_data = Dataset(X_train, y_train)
validation_data = Dataset(X_valid, y_valid)
test_data = Dataset(X_test, y_test)


def train_model(model, epochs=10, lr=0.05):
    parameters = filter(lambda p: p.requires_grad, model.parameters())
    optimizer = torch.optim.Adam(parameters, lr=lr)
    temp_best_acc = 0

    for i in range(epochs):
        tic = time.perf_counter()
        model.train()
        sum_loss = 0.0
        total = 0
        for x, y, l in train_dl:
            x = x.long()
            y = y.long()
            y_pred = model(x, l)
            optimizer.zero_grad()
            loss = F.cross_entropy(y_pred, y)
            loss.backward()
            optimizer.step()
            sum_loss += loss.item() * y.shape[0]
            total += y.shape[0]
        val_loss, val_acc, val_rmse = validation_metrics(model, val_dl)
        print("train loss %.3f, val loss %.3f, val accuracy %.3f, and val rmse %.3f" % (
        sum_loss / total, val_loss, val_acc, val_rmse))
        toc = time.perf_counter()
        print(f"The operation took {toc - tic:0.4f} seconds")

        if val_acc > temp_best_acc:
            torch.save({
                'epoch': i,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': val_loss,
                'linear': model.linear.state_dict()
            }, '../models/lstm.pt')
            torch.save({
                'model': model.state_dict()
            }, '../models/lstm_model.pt')
            temp_best_acc = val_acc
            print(temp_best_acc)


def validation_metrics(model, valid_dl):
    model.eval()
    correct = 0
    total = 0
    sum_loss = 0.0
    sum_rmse = 0.0
    for x, y, l in valid_dl:
        x = x.long()
        y = y.long()
        y_hat = model(x, l)
        loss = F.cross_entropy(y_hat, y)
        pred = torch.max(y_hat, 1)[1]
        correct += (pred == y).float().sum()
        total += y.shape[0]
        sum_loss += loss.item() * y.shape[0]
        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1))) * y.shape[0]
    return sum_loss / total, correct / total, sum_rmse / total


batch_size = 5000
vocab_size = len(words)
train_dl = DataLoader(train_data, batch_size=batch_size, shuffle=True)
val_dl = DataLoader(validation_data, batch_size=batch_size)
test_dl = DataLoader(test_data, batch_size=batch_size)


class LSTM(torch.nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.dropout = nn.Dropout(0.3)
        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0).to(device)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True).to(device)
        self.linear = nn.Linear(hidden_dim, 5).to(device)

    def forward(self, x, s):
        x = self.embeddings(x)
        x = self.dropout(x)
        x_pack = pack_padded_sequence(x, s, batch_first=True, enforce_sorted=False)
        out_pack, (ht, ct) = self.lstm(x_pack)
        out = self.linear(ht[-1])
        return out

def check_accuracy(model):
    num_correct = 0
    num_samples = 0
    model.eval()

    with torch.no_grad():
        for x, y, l in test_dl:
            x = x.long()
            y = y.long()
            y_pred = model(x, l)
            predictions = torch.max(y_pred, 1)[1]
            num_correct += (predictions == y).sum().float()
            num_samples += predictions.size(0)

        print(f'got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')

# model = LSTM_glove_vecs(vocab_size, 50, 50, pretrained_weights)
# train_model(model, epochs=30, lr=0.1)
model = LSTM(vocab_size, 60, 128).to(device)
#model = torch.load('../models/lstm_model.pt'256)
train_model(model, epochs=20, lr=0.05)

check_accuracy(model)